# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xU9RVaH8I4QcYNU60LJN5Th3Dlwulk9y
"""

# K-Means from scratch project
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# -----------------------------
# 1. Generate synthetic dataset
# -----------------------------

# 500 points, 4 true centers, 2D
X, y_true = make_blobs(
    n_samples=500,
    centers=4,
    n_features=2,
    cluster_std=0.60,
    random_state=42
)

print("Dataset shape:", X.shape)

# -----------------------------
# Helper functions
# -----------------------------

def initialize_centroids(X, K, random_state=42):
    """
    Randomly choose K points from X as initial centroids.
    """
    rng = np.random.RandomState(random_state)
    indices = rng.choice(X.shape[0], size=K, replace=False)
    centroids = X[indices]
    return centroids

def compute_distances(X, centroids):
    """
    Compute Euclidean distance from each point to each centroid.
    Returns distance matrix of shape (n_samples, K).
    """
    # X: (n, d), centroids: (K, d)
    # Use broadcasting: (n, 1, d) - (1, K, d) -> (n, K, d)
    diff = X[:, np.newaxis, :] - centroids[np.newaxis, :, :]
    distances = np.linalg.norm(diff, axis=2)
    return distances

def assign_clusters(X, centroids):
    """
    Assign each point to nearest centroid.
    Returns labels of shape (n_samples,).
    """
    distances = compute_distances(X, centroids)
    labels = np.argmin(distances, axis=1)
    return labels

def update_centroids(X, labels, K):
    """
    Recompute centroids as mean of points in each cluster.
    """
    n_features = X.shape[1]
    new_centroids = np.zeros((K, n_features))

    for k in range(K):
        cluster_points = X[labels == k]
        if len(cluster_points) > 0:
            new_centroids[k] = cluster_points.mean(axis=0)
        else:
            # Handle empty cluster: reinitialize randomly
            new_centroids[k] = X[np.random.randint(0, X.shape[0])]

    return new_centroids

def compute_sse(X, centroids, labels):
    """
    Sum of Squared Errors (SSE) for given clustering.
    """
    sse = 0.0
    for k in range(centroids.shape[0]):
        cluster_points = X[labels == k]
        if len(cluster_points) == 0:
            continue
        diff = cluster_points - centroids[k]
        sse += np.sum(diff ** 2)
    return sse

def kmeans(X, K, max_iters=100, tol=1e-4, random_state=42, verbose=False):
    """
    K-Means from scratch using NumPy.
    Returns: centroids, labels, sse_history
    """
    centroids = initialize_centroids(X, K, random_state=random_state)
    sse_history = []

    for i in range(max_iters):
        labels = assign_clusters(X, centroids)
        sse = compute_sse(X, centroids, labels)
        sse_history.append(sse)

        if verbose:
            print(f"Iteration {i+1}, SSE = {sse:.4f}")

        new_centroids = update_centroids(X, labels, K)

        # Check convergence (centroid movement)
        shift = np.linalg.norm(new_centroids - centroids)
        if shift < tol:
            if verbose:
                print(f"Converged at iteration {i+1}, shift={shift:.6f}")
            centroids = new_centroids
            break

        centroids = new_centroids

    # Final recompute SSE with final labels
    final_sse = compute_sse(X, centroids, labels)
    sse_history[-1] = final_sse

    return centroids, labels, sse_history

# -----------------------------
# 3. Elbow Method (K = 1 to 10)
# -----------------------------

K_values = range(1, 11)
sse_values = []

for K in K_values:
    print(f"Running K-Means for K = {K}")
    centroids, labels, sse_history = kmeans(
        X,
        K,
        max_iters=100,
        tol=1e-4,
        random_state=42
    )
    sse = sse_history[-1]
    sse_values.append(sse)

# Print SSE values as required deliverable
print("\nSSE values for K = 1 to 10:")
for K, sse in zip(K_values, sse_values):
    print(f"K = {K}, SSE = {sse:.4f}")

# Plot SSE vs K (Elbow plot)
plt.figure(figsize=(8, 5))
plt.plot(list(K_values), sse_values, marker='o')
plt.xticks(list(K_values))
plt.xlabel("Number of clusters (K)")
plt.ylabel("Sum of Squared Errors (SSE)")
plt.title("Elbow Method for Optimal K")
plt.grid(True)
plt.show()

# -----------------------------
# 4. Final clustering with optimal K
# -----------------------------

# From the elbow plot you will manually choose optimal_K.
# Since true centers = 4, usually elbow near 4.
optimal_K = 4  # After seeing the plot, confirm or adjust

final_centroids, final_labels, final_sse_history = kmeans(
    X,
    optimal_K,
    max_iters=100,
    tol=1e-4,
    random_state=42
)

print(f"\nChosen optimal K = {optimal_K}")
print(f"Final SSE for K={optimal_K}: {final_sse_history[-1]:.4f}")

# Visualize final clustering
plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    X[:, 0],
    X[:, 1],
    c=final_labels,
    cmap="viridis",
    s=25,
    alpha=0.8
)
plt.scatter(
    final_centroids[:, 0],
    final_centroids[:, 1],
    c="red",
    s=200,
    marker="X",
    edgecolors="black",
    linewidths=2,
    label="Centroids"
)
plt.title(f"K-Means Clustering (K = {optimal_K})")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()